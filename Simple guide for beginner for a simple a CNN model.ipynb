{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A guide to start a simple CNN model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Flatten=nn.Flatten()\n",
    "        self.Dense=nn.Sequential(\n",
    "            nn.Linear(28*28,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,10)\n",
    "        )\n",
    "        self.Softmax=nn.Softmax(dim=1)\n",
    "    def forward(self, inputs):\n",
    "        Flattened=self.Flatten(inputs)\n",
    "        logits=self.Dense(Flattened)\n",
    "        predictions=self.Softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, model, train_dataloader, loss_fn, optimizer):\n",
    "    loss_epoch=[]\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        total_loss=0\n",
    "        j=0\n",
    "    \n",
    "        for inputs, targets in train_dataloader:\n",
    "            predictions=model(inputs)\n",
    "            loss=loss_fn(predictions, targets)\n",
    "            total_loss=total_loss+loss\n",
    "            j=j+1\n",
    "        loss_epoch.append(total_loss/j)\n",
    "        print(f\"Epoch={i+1} and Loss={total_loss/j}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, test_dataloader, loss_fn,class_mapping):\n",
    "    accuracy_t=[]\n",
    "    for test_inputs, test_targets in test_dataloader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions=model(test_inputs)\n",
    "            predicted_index=predictions.argmax(1)\n",
    "            matching=torch.sum(predicted_index==test_targets)\n",
    "            accuracy=matching/len(predicted_index)\n",
    "            predicted=class_mapping[predicted_index[0]]\n",
    "            expected=class_mapping[test_targets[0]]\n",
    "            loss=loss_fn(predictions, test_targets)\n",
    "            accuracy_t.append(accuracy)\n",
    "    return(np.mean(accuracy_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping=[\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    train_data=datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "    test_data=datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "    return train_data, train_data\n",
    "\n",
    "train_data, test_data=download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader=DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1 and Loss=2.3021485805511475\n",
      "Epoch=2 and Loss=2.294346332550049\n",
      "Epoch=3 and Loss=2.287260055541992\n",
      "Epoch=4 and Loss=2.2781646251678467\n",
      "Epoch=5 and Loss=2.2671656608581543\n"
     ]
    }
   ],
   "source": [
    "model=train_model(epochs, model, train_dataloader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32379398"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model(model, test_dataloader, loss_fn, class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
